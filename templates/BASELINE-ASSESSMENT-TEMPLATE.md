# Baseline Assessment

> Use this template to measure your PIV Loop improvement over time.
> Complete it BEFORE starting the system (baseline) and AFTER completing core guides (comparison).
> Track the same feature both times for a valid comparison.
>
> **Core Principle**: The system works if all five categories improve together.
> If time decreased but confidence dropped, you optimized for speed, not quality.

---

## Feature Being Assessed

- **Feature Name**: {name}
- **Complexity**: {Low / Medium / High}
- **Description**: {One sentence â€” what does this feature do?}
- **Assessment Type**: {Baseline / Post-System}
- **Date**: {YYYY-MM-DD}

---

## Category 1: Time Tracking

| Metric | Value |
|--------|-------|
| Backend implementation time | ___ minutes |
| Frontend implementation time | ___ minutes |
| Total time (including testing, review) | ___ minutes |

## Category 2: AI Interaction

| Metric | Value |
|--------|-------|
| Number of prompts | ___ |
| Types of prompts | {code generation, debugging, explanation, refactor} |
| Iteration cycles (back-and-forth exchanges) | ___ |
| Time waiting for AI responses | ___ minutes |
| Time reviewing/editing AI output | ___ minutes |

## Category 3: Confidence Levels

| Metric | Rating (1-10) |
|--------|---------------|
| Code is **correct** | ___ /10 |
| Code follows **best practices** | ___ /10 |
| I **understand** the generated code | ___ /10 |
| Code is **maintainable** | ___ /10 |

## Category 4: Issues Encountered

| Metric | Value |
|--------|-------|
| AI mistakes (describe types) | ___ |
| Type errors or test failures | ___ |
| Debugging cycles required | ___ |
| Manual rework needed | ___ % |

## Category 5: Quality Signals

| Check | Result |
|-------|--------|
| Tests pass | Yes / No |
| Compiles/runs without errors | Yes / No |
| Feature works end-to-end | Yes / No |
| Would pass basic code review | Yes / No |

---

## Honest Self-Assessment Checklist

Before finalizing, verify:

- [ ] I worked at normal pace (not rushing or overthinking)
- [ ] I tracked time accurately (with breaks excluded)
- [ ] I counted all prompts (including follow-ups)
- [ ] I rated confidence realistically (not inflated)
- [ ] I documented all issues (not just major ones)
- [ ] This is a real feature, not a toy problem
- [ ] I would be comfortable sharing these numbers

If any checkbox is uncertain, re-run the assessment.

---

## Improvement Comparison

> Fill this section only on the Post-System assessment.

| Category | Before (Baseline) | After (Post-System) | Change |
|----------|-------------------|---------------------|--------|
| Total time | ___ min | ___ min | {target: 30-50% reduction} |
| Number of prompts | ___ | ___ | {target: 40-60% fewer} |
| Avg confidence | ___ /10 | ___ /10 | {target: +2-3 points} |
| Debugging cycles | ___ | ___ | {target: fewer} |
| Quality signals passing | ___ /4 | ___ /4 | {target: 4/4} |

**Expected improvements**: Time 30-50% less, prompts 40-60% fewer, confidence +2-3 points, fewer bugs.

---

> **Reference**: See `reference/system-foundations.md` Section 3 for detailed explanations of each category and measurement methodology.
